# -*- coding: utf-8 -*-
"""BİTİRME ÇALIŞMASI II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8NcEfbY66fGUM1hAp1FaXVDQUKXMkmX
"""

# Gerekli kütüphaneleri yükleme
!pip install jsonlines
!apt-get update -qq
!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://github.com/ahmetaa/zemberek-nlp/releases/download/v0.17.1/zemberek-full.jar -O zemberek-full.jar
!pip install py4j
!pip install jpype1
!pip install zemberek-python


import jsonlines
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import TruncatedSVD
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight
from zemberek import TurkishMorphology
from py4j.java_gateway import JavaGateway
import jpype
import jpype.imports
from jpype.types import JString
import jsonlines

from google.colab import drive

# Google Drive'ı bağla
drive.mount('/content/drive')

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.nn.functional import softmax

# Google Drive'ı bağlama
from google.colab import drive
drive.mount('/content/drive')

# GPU kullanımı kontrolü
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

# Dosya yolları
train_data_path = "/content/drive/My Drive/zemberturk_embeddings.csv"
test_data_path = "/content/drive/My Drive/zemberturk_embeddings_test.csv"

# Eğitim ve Test Verilerini Yükleme
train_df = pd.read_csv(train_data_path)
test_df = pd.read_csv(test_data_path)

print("Eğitim veri seti başarıyla yüklendi!")
print("Test veri seti başarıyla yüklendi!")
print(train_df.head())
print(test_df.head())

# Embedding ve Etiketlerin Hazırlanması
train_embedding_df = train_df[[col for col in train_df.columns if col.startswith("embedding_")]]
test_embedding_df = test_df[[col for col in test_df.columns if col.startswith("embedding_")]]

X_train = train_embedding_df.values
y_train = train_df['Offensive_Label'].values

X_test = test_embedding_df.values
y_test = test_df['Offensive_Label'].values

# PyTorch Dataset Sınıfı
class EmbeddingDataset(Dataset):
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)

train_dataset = EmbeddingDataset(X_train, y_train)
test_dataset = EmbeddingDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128)

# Model Sınıfları
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)  # Batch boyutuna uygun hale getirme
        _, (hidden, _) = self.lstm(x)
        return self.fc(hidden[-1])

class BiLSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(BiLSTMClassifier, self).__init__()
        self.bilstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)
        _, (hidden, _) = self.bilstm(x)
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        return self.fc(hidden)

class GRUClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(GRUClassifier, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)
        _, hidden = self.gru(x)
        return self.fc(hidden[-1])

class CNNClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(CNNClassifier, self).__init__()
        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.fc = nn.Linear((input_size // 2) * 128, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)  # Add channel dimension
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # Flatten
        return self.fc(x)

# Eğitim Fonksiyonu
def train_model(model, train_loader, criterion, optimizer, epochs):
    model.to(device)  # Modeli GPU'ya taşı
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        correct_train_preds = 0
        total_train_preds = 0
        for embeddings, labels in train_loader:
            embeddings, labels = embeddings.to(device), labels.to(device)  # Veriyi GPU'ya taşı
            optimizer.zero_grad()
            outputs = model(embeddings)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct_train_preds += torch.sum(preds == labels).item()
            total_train_preds += labels.size(0)

        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(correct_train_preds / total_train_preds)

        # Validation loss and accuracy
        model.eval()
        val_loss = 0
        correct_val_preds = 0
        total_val_preds = 0
        with torch.no_grad():
            for embeddings, labels in test_loader:
                embeddings, labels = embeddings.to(device), labels.to(device)
                outputs = model(embeddings)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                correct_val_preds += torch.sum(preds == labels).item()
                total_val_preds += labels.size(0)

        val_losses.append(val_loss / len(test_loader))
        val_accuracies.append(correct_val_preds / total_val_preds)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.4f} - Train Accuracy: {train_accuracies[-1]:.4f} - Val Loss: {val_losses[-1]:.4f} - Val Accuracy: {val_accuracies[-1]:.4f}")

    return train_losses, val_losses, train_accuracies, val_accuracies

# Eğitim ve doğrulama kayıpları ve doğrulukları grafiği çizme
def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, model_name):
    epochs = range(1, len(train_losses) + 1)

    # Kayıp grafiği
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, val_losses, label="Validation Loss")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'{model_name} - Train & Validation Loss')
    plt.legend()

    # Doğruluk grafiği
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label="Train Accuracy")
    plt.plot(epochs, val_accuracies, label="Validation Accuracy")
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'{model_name} - Train & Validation Accuracy')
    plt.legend()

    # Grafiklerin gösterilmesi
    plt.tight_layout()
    plt.show()

# Evaluate on train data
def evaluate_on_train(model, train_loader):
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for embeddings, labels in train_loader:
            embeddings, labels = embeddings.to(device), labels.to(device)
            outputs = model(embeddings)
            _, preds = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    return all_labels, all_preds

# Evaluate on test data
def evaluate_on_test(model, test_loader):
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for embeddings, labels in test_loader:
            embeddings, labels = embeddings.to(device), labels.to(device)
            outputs = model(embeddings)
            _, preds = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    return all_labels, all_preds

# Print classification metrics (classification report and confusion matrix)
def print_classification_metrics(true_labels, preds, model_name, data_type="Train"):
    print(f"\n{model_name} Modeli - {data_type} Verisi Değerlendirme:")
    # Classification report
    print(classification_report(true_labels, preds))

    # Confusion matrix
    cm = confusion_matrix(true_labels, preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-Offensive', 'Offensive'], yticklabels=['Non-Offensive', 'Offensive'])
    plt.title(f"{model_name} - {data_type} Confusion Matrix")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Model Parametreleri
input_size = train_embedding_df.shape[1]  # Embedding boyutu
hidden_size = 128
num_classes = len(train_df['Offensive_Label'].unique())
learning_rate = 0.0001
epochs = 30

# Modellerin Eğitimi ve Değerlendirilmesi
for ModelClass, model_name in zip([LSTMClassifier, BiLSTMClassifier, GRUClassifier, CNNClassifier], ["LSTM", "Bi-LSTM", "GRU", "CNN"]):
    print(f"\n{model_name} Modeli Eğitimde...")
    model = ModelClass(input_size, hidden_size, num_classes) if model_name != "CNN" else ModelClass(input_size, num_classes)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Eğitim Süreci
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, criterion, optimizer, epochs)

    # Eğitim Verisi Üzerinde Tahminler ve Confusion Matrix
    train_labels, train_preds = evaluate_on_train(model, train_loader)
    print_classification_metrics(train_labels, train_preds, model_name, data_type="Train")

    # Test Verisi Üzerinde Tahminler ve Confusion Matrix
    test_labels, test_preds = evaluate_on_test(model, test_loader)
    print_classification_metrics(test_labels, test_preds, model_name, data_type="Test")

    # Doğruluk ve Kayıp Grafiklerini Çizme
    plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies, model_name)

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import torch.nn.functional as F  # softmax fonksiyonunu kullanabilmek için

# Confusion Matrix Fonksiyonu
def plot_confusion_matrix(y_true, y_pred, num_classes, title):
    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Sınıflandırma Raporları Fonksiyonu
def print_classification_results(model_name, train_labels, train_preds, test_labels, test_preds):
    print(f"{model_name} Modeli için Eğitim Verisi Sınıflandırma Raporu:")
    print(classification_report(train_labels, train_preds))
    print(f"{model_name} Modeli için Test Verisi Sınıflandırma Raporu:")
    print(classification_report(test_labels, test_preds))

# GPU kullanımı
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Veri Yükleme
train_data_path = "/content/drive/My Drive/zemberturk_embeddings.csv"
test_data_path = "/content/drive/My Drive/zemberturk_embeddings_test.csv"

# Dosyaları yükle
train_df = pd.read_csv(train_data_path)
test_df = pd.read_csv(test_data_path)

# DistilBERT Tokenizer ve Model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Özel kelimeler listesi
custom_words = ["şerefsiz", "aptal", "salak", "kötü", "çirkin", "nefret",
                "gerizekalı", "haksız", "manyak", "amk", "aq", "iğrenç"]

# Tokenizer'a özel kelimeleri ekle
tokenizer.add_tokens(custom_words)

# DistilBERT Modeli
distilbert_model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=len(train_df['Offensive_Label'].unique())
)

# Modelin embedding katmanını yeniden boyutlandır
distilbert_model.resize_token_embeddings(len(tokenizer))
distilbert_model.to(device)

# Metin ve Etiketlerin Hazırlanması
train_texts = train_df['text'].values
train_labels = train_df['Offensive_Label'].values

test_texts = test_df['text'].values
test_labels = test_df['Offensive_Label'].values

# Eğitim ve doğrulama setini ayırırken %15 doğrulama seti almak için `train_test_split` fonksiyonunu doğru kullan
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.15, random_state=42, stratify=train_labels
)

# Eğitim ve doğrulama setindeki sınıf dağılımını kontrol et
print(f"Train set sınıf 0 örnekleri: {sum(train_labels == 0)}")
print(f"Train set sınıf 1 örnekleri: {sum(train_labels == 1)}")
print(f"Validation set sınıf 0 örnekleri: {sum(val_labels == 0)}")
print(f"Validation set sınıf 1 örnekleri: {sum(val_labels == 1)}")

# Toplam veri kümesindeki örnek sayısı
print(f"Toplam eğitim + doğrulama seti büyüklüğü: {len(train_texts) + len(val_texts)}")

# DistilBERT Dataset ve DataLoader
class DistilBERTDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors="pt",
            return_attention_mask=True
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Dataset ve DataLoader'ları oluştur
train_dataset = DistilBERTDataset(train_texts, train_labels, tokenizer)
val_dataset = DistilBERTDataset(val_texts, val_labels, tokenizer)
test_dataset = DistilBERTDataset(test_texts, test_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)

# DistilBERT Eğitim Fonksiyonu (Aynı kalıyor)
def train_distilbert(model, train_loader, val_loader, optimizer, epochs):
    train_losses = []
    val_losses = []
    train_accuracies = []  # Eğitim doğrulukları için liste
    val_accuracies = []    # Validation doğrulukları için liste

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        correct_train = 0
        total_train = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

            # Doğruluk hesaplama
            _, predicted = torch.max(outputs.logits, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        # Validation
        model.eval()
        val_loss = 0
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()

                # Validation doğruluk hesaplama
                _, predicted = torch.max(outputs.logits, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        # Hesaplamalar
        train_accuracy = 100 * correct_train / total_train
        val_accuracy = 100 * correct_val / total_val

        # Listelere ekleme
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss / len(val_loader))
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)

        print(f"Epoch {epoch+1}/{epochs}")
        print(f"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_accuracy:.2f}%")
        print(f"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_accuracy:.2f}%")
        print("-" * 50)

    # Loss ve Accuracy grafikleri
    plt.figure(figsize=(12, 5))

    # Loss Grafiği
    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    # Accuracy Grafiği
    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

    return model

# DistilBERT Değerlendirme Fonksiyonu (Aynı kalıyor)


def evaluate_distilbert(model, loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for batch in loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            # softmax fonksiyonunu F.softmax ile kullanıyoruz
            preds = torch.argmax(F.softmax(outputs.logits, dim=1), dim=1).cpu().numpy()

            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    return all_labels, all_preds


# Model Parametreleri ve Eğitim
optimizer = torch.optim.AdamW(distilbert_model.parameters(), lr=2e-5)
epochs = 3

print("\nDistilBERT Modeli Eğitimde...")
distilbert_model = train_distilbert(distilbert_model, train_loader, val_loader, optimizer, epochs)

# Performans Değerlendirmesi
train_labels, train_preds = evaluate_distilbert(distilbert_model, train_loader)
test_labels, test_preds = evaluate_distilbert(distilbert_model, test_loader)

# Sonuçları Yazdır
print_classification_results("DistilBERT", train_labels, train_preds, test_labels, test_preds)
plot_confusion_matrix(train_labels, train_preds, len(train_df['Offensive_Label'].unique()), 'DistilBERT Modeli - Train Verisi Confusion Matrix')
plot_confusion_matrix(test_labels, test_preds, len(train_df['Offensive_Label'].unique()), 'DistilBERT Modeli - Test Verisi Confusion Matrix')

# Gerekli kütüphaneleri içeri aktar
import pandas as pd
from google.colab import drive
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
from lime.lime_text import LimeTextExplainer
import numpy as np
import torch.nn.functional as F  # softmax fonksiyonunu kullanabilmek için

# Google Drive'ı bağla
drive.mount('/content/drive')

# DistilBERT Tokenizer ve Modeli
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=2  # Burada sadece iki etiket olduğunu varsayıyoruz (Offensive ve Not Offensive)
)

# Modeli GPU'ya taşı (varsa)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
distilbert_model.to(device)

# LIME Text Explainer oluşturulması
explainer = LimeTextExplainer(class_names=['Not Offensive', 'Offensive'])

# LIME için model tahmin fonksiyonu
def predict_proba(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    # Model tahminini al
    with torch.no_grad():
        outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1).cpu().numpy()  # softmax ile olasılıkları al
    return probs

# Kendinizin yazdığı metin örneği
sample_text = "Bu ürün gerçekten çok kötü, almanızı tavsiye etmiyorum."  # Burada örnek bir metin yazdık

# LIME açıklaması oluşturulması
explanation = explainer.explain_instance(sample_text, predict_proba, num_features=10)

# Özelliklerin (features) ve bunların modelin tahminine etkilerini yazdırma
explanation.show_in_notebook()  # Bu, Jupyter notebook üzerinde görsel olarak açıklamayı gösterecek

# Alternatif olarak açıklamayı yazdırma
print("Explanation for text:", sample_text)
for feature, weight in explanation.as_list():
    print(f"{feature}: {weight}")