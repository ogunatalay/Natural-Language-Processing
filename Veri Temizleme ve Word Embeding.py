# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

# Gerekli kütüphaneleri yükleme
!pip install jsonlines
!apt-get update -qq
!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://github.com/ahmetaa/zemberek-nlp/releases/download/v0.17.1/zemberek-full.jar -O zemberek-full.jar
!pip install py4j
!pip install jpype1
!pip install zemberek-python


import jsonlines
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight
from zemberek import TurkishMorphology
from py4j.java_gateway import JavaGateway
import jpype
import jpype.imports
from jpype.types import JString
import jsonlines


from google.colab import drive

# Google Drive'ı bağlayalım
drive.mount('/content/drive')

# Dosya yollarını belirleyelim
train_file_path = '/content/drive/MyDrive/zemcleaned_data_test.csv'
test_file_path = '/content/drive/MyDrive/zemcleaned_data.csv'

# CSV dosyalarını yükleyelim
train_df = pd.read_csv(train_file_path)
test_df = pd.read_csv(test_file_path)

# Train ve test verilerindeki etiketlerin sayısını alalım
train_label_counts = train_df['label'].value_counts()
test_label_counts = test_df['label'].value_counts()

# Grafik oluşturma
plt.figure(figsize=(10, 5))

# Train verisinin grafiği
plt.subplot(1, 2, 1)
train_label_counts.plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Test Data - Label Counts')
plt.xlabel('Label')
plt.ylabel('Count')

# Test verisinin grafiği
plt.subplot(1, 2, 2)
test_label_counts.plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Train Data - Label Counts')
plt.xlabel('Label')
plt.ylabel('Count')

# Grafiklerin gösterilmesi
plt.tight_layout()
plt.show()

import pandas as pd
from zemberek import TurkishMorphology
from zemberek.tokenization import TurkishTokenizer
import re

# Zemberek'i başlat
morphology = TurkishMorphology.create_with_defaults()
tokenizer = TurkishTokenizer.DEFAULT

# Stopwords ve link temizliği
stop_words = {'https', 'ol', 'co', 'bir', 'var', 't', 've', 'bu', 'da', 'de',
             'için', 'olarak', 'ama', 'yani', 'çok'}

# Küfürlü kısaltmalar listesi
offensive_abbreviations = {'amk', 'aq'}

def remove_stopwords_links_and_hashes(input_text):
    input_text = re.sub(r'http\S+|www\S+', '', input_text)  # Linkleri çıkar
    input_text = re.sub(r'#\S+', '', input_text)           # '#' ile başlayan ifadeleri çıkar
    input_text = re.sub(r'@\S+', '', input_text)           # '@' ile başlayan ifadeleri çıkar
    input_text = ' '.join(word for word in input_text.split()
                         if len(word) > 1 and word not in stop_words)
    return input_text

# Küfürlü kısaltmaları ve kelimeleri metinden çıkarma (Metni özel işleme)
def keep_offensive_abbreviations(input_text):
    # Küfürlü kısaltmaları metinden çıkarma (bunu geçici olarak yapıyoruz)
    for abbrev in offensive_abbreviations:
        input_text = input_text.replace(abbrev, f" {abbrev} ")  # Kısaltmaları belirgin hale getir
    return input_text

# Metinleri ön işleme
def preprocess_text(input_text):
    input_text = input_text.lower()  # Küçük harfe çevir
    input_text = keep_offensive_abbreviations(input_text)  # Küfürlü kısaltmaları metne dahil et

    tokens = tokenizer.tokenize(input_text)  # Tokenize et
    stems = []
    for token in tokens:
        # Kök çözümleme, yalnızca normal kelimeler için yapılacak
        if token.normalized not in offensive_abbreviations:  # Kısaltmaları işlem dışı bırak
            analyses = morphology.analyze(token.normalized)  # Kökleri bul
            if analyses.analysis_results:
                best_analysis = analyses.analysis_results[0]
                stems.append(best_analysis.get_stem())  # En iyi analizi al
        else:
            # Kısaltma metne olduğu gibi eklenir
            stems.append(token.normalized)

    # Kısaltmaları metne geri ekle
    processed_text = ' '.join(stems)

    return processed_text

# Metinleri temizle ve işleme al
df = pd.read_json("/content/drive/My Drive/train.jsonlines", lines=True)
df['Processed_Text'] = df['text'].apply(preprocess_text)
df['Processed_Text'] = df['Processed_Text'].apply(remove_stopwords_links_and_hashes)

# Etiketleri sayısal değerlere dönüştürme
df['Offensive_Label'] = df['label'].map({'not-offensive': 0, 'offensive': 1})

# Sonuçları kontrol et
print(df[['text', 'Processed_Text', 'label', 'Offensive_Label']].head())

# Temizlenmiş veriyi Google Drive'a kaydet
output_file_path = '/content/drive/My Drive/zemcleaned_data.csv'  # Kaydetmek istediğiniz tam yol
df.to_csv(output_file_path, index=False)

print(f"Temizlenmiş veri CSV dosyasına kaydedildi: {output_file_path}")

# Gerekli kütüphaneleri yükleyin
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import numpy as np
import torch

# GPU kullanılabilir mi kontrol et
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU kullanılabilir. PyTorch modeli GPU'ya taşındı.")
else:
    device = torch.device("cpu")
    print("GPU kullanılmıyor. PyTorch modeli CPU'ya taşındı.")

# Google Drive'ı bağla (sadece Google Colab için)
drive.mount('/content/drive')

# Veri setini yükle
file_path = '/content/drive/My Drive/zemcleaned_data.csv'  # Dosyanızın tam yolunu belirtin
df = pd.read_csv(file_path)

# Etiketlerin Şifrelenmesi
label_encoder = LabelEncoder()
df['Offensive_Label'] = label_encoder.fit_transform(df['label'])

print("Şifrelenmiş Etiketler:")
print(df[['label', 'Offensive_Label']].drop_duplicates())

# BERTurk Tokenizer ve Modelini Yükleyin
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")
model = AutoModel.from_pretrained("dbmdz/bert-base-turkish-cased")

# Metinleri Embedding'e çevirme fonksiyonu
def get_bert_embeddings(texts):
    embeddings = []
    for text in tqdm(texts, desc="Embedding İşlemi"):
        # Metnin string olup olmadığını ve boş olmadığını kontrol et
        if isinstance(text, str) and text:
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            with torch.no_grad():
                outputs = model(**inputs)
            # [CLS] token çıkışları (Cümle Embedding'i)
            embeddings.append(outputs.last_hidden_state[:, 0, :].squeeze().numpy())
        else:
            # String olmayan veya boş metinleri işle (örneğin, boş bir embedding ile değiştir)
            embeddings.append(np.zeros(model.config.hidden_size))
    return embeddings

# Temizlenmiş metinleri Embedding'e dönüştürme
texts = df['Processed_Text'].tolist()
embeddings = get_bert_embeddings(texts)

# Embedding'leri DataFrame'e ekleme
embedding_df = pd.DataFrame(embeddings)
embedding_df.columns = [f"embedding_{i}" for i in range(embedding_df.shape[1])]
df = pd.concat([df, embedding_df], axis=1)

# Sonuçları kaydetme
output_file = "berturk_embeddings.csv"
df.to_csv(output_file, index=False)
print(f"Embedding sonuçları {output_file} dosyasına kaydedildi.")

from google.colab import drive
drive.mount('/content/drive')
output_file = "/content/drive/My Drive/zemberturk_embeddings.csv"
df.to_csv(output_file, index=False)
print(f"Embedding sonuçları {output_file} dosyasına kaydedildi.")

import pandas as pd
from zemberek import TurkishMorphology
from zemberek.tokenization import TurkishTokenizer
import re

# Zemberek'i başlat
morphology = TurkishMorphology.create_with_defaults()
tokenizer = TurkishTokenizer.DEFAULT

# Stopwords ve link temizliği
stop_words = {'https', 'ol', 'co', 'bir', 'var', 't', 've', 'bu', 'da', 'de',
             'için', 'olarak', 'ama', 'yani', 'çok'}

# Küfürlü kısaltmalar listesi
offensive_abbreviations = {'amk', 'aq'}

def remove_stopwords_links_and_hashes(input_text):
    input_text = re.sub(r'http\S+|www\S+', '', input_text)  # Linkleri çıkar
    input_text = re.sub(r'#\S+', '', input_text)           # '#' ile başlayan ifadeleri çıkar
    input_text = re.sub(r'@\S+', '', input_text)           # '@' ile başlayan ifadeleri çıkar
    input_text = ' '.join(word for word in input_text.split()
                         if len(word) > 1 and word not in stop_words)
    return input_text

# Küfürlü kısaltmaları ve kelimeleri metinden çıkarma (Metni özel işleme)
def keep_offensive_abbreviations(input_text):
    # Küfürlü kısaltmaları metinden çıkarma (bunu geçici olarak yapıyoruz)
    for abbrev in offensive_abbreviations:
        input_text = input_text.replace(abbrev, f" {abbrev} ")  # Kısaltmaları belirgin hale getir
    return input_text

# Metinleri ön işleme
def preprocess_text(input_text):
    input_text = input_text.lower()  # Küçük harfe çevir
    input_text = keep_offensive_abbreviations(input_text)  # Küfürlü kısaltmaları metne dahil et

    tokens = tokenizer.tokenize(input_text)  # Tokenize et
    stems = []
    for token in tokens:
        # Kök çözümleme, yalnızca normal kelimeler için yapılacak
        if token.normalized not in offensive_abbreviations:  # Kısaltmaları işlem dışı bırak
            analyses = morphology.analyze(token.normalized)  # Kökleri bul
            if analyses.analysis_results:
                best_analysis = analyses.analysis_results[0]
                stems.append(best_analysis.get_stem())  # En iyi analizi al
        else:
            # Kısaltma metne olduğu gibi eklenir
            stems.append(token.normalized)

    # Kısaltmaları metne geri ekle
    processed_text = ' '.join(stems)

    return processed_text

# Metinleri temizle ve işleme al
df = pd.read_json("/content/drive/My Drive/test.jsonlines", lines=True)
df['Processed_Text'] = df['text'].apply(preprocess_text)
df['Processed_Text'] = df['Processed_Text'].apply(remove_stopwords_links_and_hashes)

# Etiketleri sayısal değerlere dönüştürme
df['Offensive_Label'] = df['label'].map({'not-offensive': 0, 'offensive': 1})

# Sonuçları kontrol et
print(df[['text', 'Processed_Text', 'label', 'Offensive_Label']].head())

# Temizlenmiş veriyi Google Drive'a kaydet
output_file_path = '/content/drive/My Drive/zemcleaned_data_test.csv'  # Kaydetmek istediğiniz tam yol
df.to_csv(output_file_path, index=False)

print(f"Temizlenmiş veri CSV dosyasına kaydedildi: {output_file_path}")
